{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# import sklearn for accuracy, F1 score and AUC calcs as per update 24/04/2021\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "test_data = np.load('test_data.npy')\n",
    "test_label = np.load('test_label.npy')\n",
    "train_data = np.load('train_data.npy')\n",
    "train_label = np.load('train_label.npy')\n",
    "\n",
    "# Output training data to one hot encoding\n",
    "nb_classes = 10\n",
    "targets = train_label.reshape(-1)\n",
    "one_hot_targets = np.eye(nb_classes)[targets]\n",
    "one_hot_targets_test = np.eye(nb_classes)[test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function outputing the run time in minutes & secs\n",
    "def timeValue(time_input):\n",
    "    if time_input < 60:\n",
    "        time_output = str(round(time_input, 1)) + ' seconds'\n",
    "    else:\n",
    "        time_output = str(int(time_input // 60)) + ' minutes, ' + str(round(time_input % 60, 1) - 1) + ' seconds'\n",
    "    return time_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-max normalization for the test and train data sets since input data is not norm distributed\n",
    "train_data = (train_data - np.amin(train_data,axis=0, keepdims=True))\\\n",
    "            / (np.amax(train_data,axis=0, keepdims=True)- np.amin(train_data,axis=0, keepdims=True))\n",
    "test_data = (test_data - np.amin(test_data,axis=0, keepdims=True))\\\n",
    "            / (np.amax(test_data,axis=0, keepdims=True)- np.amin(test_data,axis=0, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "class Activation(object):\n",
    "    \n",
    "    # Tanh activation function\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        return 1.0 - a**2\n",
    "    \n",
    "    # Logistic/sigmoid activation function\n",
    "    def __logistic(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        return  a * (1 - a )\n",
    "    \n",
    "    # ReLU activation function\n",
    "    # For ReLU and Leaky ReLU, the derivatrive is undefined at a=0, but in practise this is usually ignored\n",
    "    def __relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def __relu_deriv(self, a): \n",
    "        return np.where(a <= 0, 0, 1)\n",
    "    \n",
    "    # Leaky ReLU activation function\n",
    "    # For Leaky ReLU the convetion is to use 0.01 for the gradient.\n",
    "    def __leaky_relu(self, x):\n",
    "        return np.maximum(0.01* x , x)\n",
    "    \n",
    "    def __leaky_relu_deriv(self, a): \n",
    "        return np.where(a <=0, 0.01, 1)\n",
    "    \n",
    "    # softmax activation function\n",
    "    def __softmax(self, x):\n",
    "        x = x - np.max(x)\n",
    "        return np.exp(x) / (np.sum(np.exp(x) + 1e-15, axis=0))    \n",
    "\n",
    "    def __softmax_deriv(self, a):\n",
    "        return a * (1 - a)\n",
    "    \n",
    "    # activation function class\n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "            \n",
    "        elif activation == 'relu':\n",
    "            self.f = self.__relu\n",
    "            self.f_deriv = self.__relu_deriv\n",
    "    \n",
    "        elif activation == 'leaky_relu':\n",
    "            self.f = self.__leaky_relu\n",
    "            self.f_deriv = self.__leaky_relu_deriv\n",
    "        \n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        \n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__softmax_deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalisation forward pass\n",
    "def batch_norm_forward_pass(x, beta, gamma, step_type, running_mean=None, running_var=None, epsilon=1e-5):\n",
    "    if step_type == 'train':\n",
    "        mean = x.mean(axis=0)\n",
    "        var = x.var(axis=0)\n",
    "        std = np.sqrt(var + epsilon)\n",
    "\n",
    "        x_centered = x - mean\n",
    "        x_norm = x_centered / std\n",
    "        out = gamma * x_norm + beta\n",
    "\n",
    "        return out, mean, var, (x_norm, x_centered, gamma, std)\n",
    "    \n",
    "    std = np.sqrt(running_var + epsilon)\n",
    "    x_centered = x - running_mean\n",
    "    x_norm = x_centered / std\n",
    "    out = gamma * x_norm + beta\n",
    "    \n",
    "    return out\n",
    "    \n",
    "# Batch normalisation backward pass\n",
    "def batch_norm_backward_pass(dx, backprop_data):\n",
    "    x_norm, x_centered, gamma, std = backprop_data\n",
    "    \n",
    "    dgamma = (dx * x_norm).sum(axis=0)\n",
    "    dbeta = dx.sum(axis=0)\n",
    "    dx_norm = dx * gamma\n",
    "    dx = 1 / dx.shape[0] / std * (dx.shape[0] * dx_norm - dx_norm.sum(axis=0) - x_norm * (dx_norm * x_norm).sum(axis=0))\n",
    "    \n",
    "    return dx, dbeta, dgamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layer function\n",
    "class HiddenLayer(object):    \n",
    "    \n",
    "    def __init__(self, n_in, n_out, activation_last_layer='tanh',activation='tanh', W=None, b=None):\n",
    "\n",
    "        self.input = None\n",
    "    \n",
    "        self.activation = None\n",
    "        if activation:\n",
    "            self.activation = Activation(activation).f\n",
    "        \n",
    "        # Activation deriv of last layer\n",
    "        self.activation_deriv = None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv = Activation(activation_last_layer).f_deriv\n",
    "        \n",
    "        # Weight initialisation for relu and leaky relu is He     \n",
    "        if activation == 'relu' or activation == 'leaky_relu':\n",
    "            self.W = np.random.uniform(\n",
    "                low =- np.sqrt(2. / n_in),\n",
    "                high = np.sqrt(2. / n_in),\n",
    "                size = (n_in, n_out)\n",
    "            ) \n",
    "        else:\n",
    "            self.W = np.random.uniform(\n",
    "                low =- np.sqrt(6. / (n_in + n_out)),\n",
    "                high = np.sqrt(6. / (n_in + n_out)),\n",
    "                size = (n_in, n_out)\n",
    "            )\n",
    "            \n",
    "        # Weight initialisation for logistic is 4x xavier   \n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "                    \n",
    "        # Set the size of bias as the size of output dimension\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        # Set the size of weight grad\n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "        # Set the size of the accum grad for momentum calc\n",
    "        self.accgrad_W = np.zeros(self.W.shape)\n",
    "        self.accgrad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "        # Set the size of the beta, gamma, running mean/var, backprop_data for the batch normalisation\n",
    "        self.beta = np.random.randn(n_out,)\n",
    "        self.gamma = np.random.randn(n_out,)\n",
    "        self.running_mean = np.zeros(n_out,)\n",
    "        self.running_var = np.zeros(n_out,)\n",
    "        self.backprop_data = None\n",
    "    \n",
    "    # Forward pass for each training epoch\n",
    "    def forward(self, input, step_type, batch_norm, momentum=None):\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        \n",
    "        if batch_norm:\n",
    "            if step_type == 'train':\n",
    "                lin_output, mean, var, self.backprop_data = batch_norm_forward_pass(lin_output, self.beta, self.gamma, 'train')\n",
    "\n",
    "                self.running_mean = momentum * self.running_mean + (1 - momentum) * mean\n",
    "                self.running_var = momentum * self.running_var + (1 - momentum) * var\n",
    "            elif step_type == 'test':\n",
    "                lin_output = batch_norm_forward_pass(lin_output, self.beta, self.gamma, 'test', self.running_mean, self.running_var)\n",
    "            \n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        \n",
    "        self.input=input\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    # Backward pass for each training epoch\n",
    "    def backward(self, delta, batch_norm, output_layer=False):\n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = np.dot(np.ones((self.input.shape[0], 1)).T, delta)\n",
    "\n",
    "        if self.activation_deriv:\n",
    "            if batch_norm:\n",
    "                delta = delta * self.activation_deriv(self.output)\n",
    "                delta, dbeta, dgamma = batch_norm_backward_pass(delta, self.backprop_data)\n",
    "                delta = delta.dot(self.W.T)\n",
    "                self.gamma += dgamma\n",
    "                self.beta += dbeta\n",
    "            else:\n",
    "                delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Model\n",
    "class MLP:\n",
    "\n",
    "    # for initiallization, the code will create all layers automatically based on the provided parameters.     \n",
    "    def __init__(self, layers, activation=[None,'tanh','tanh'], batch_norm= False):\n",
    "      \n",
    "        # initialize layers\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        \n",
    "        # initialize dropout & running mean/var (for batch norm)\n",
    "        self.dropout = 1\n",
    "        self.running_mean = None\n",
    "        self.running_var = None\n",
    "        self.epochs = None\n",
    "        self.batch_size = None\n",
    "        \n",
    "        self.activation = activation\n",
    "        for i in range(len(layers)-1):\n",
    "            if batch_norm:\n",
    "                self.layers.append(HiddenLayer(layers[i], layers[i + 1], activation[i], activation[i + 1])) \n",
    "            else:\n",
    "                self.layers.append(HiddenLayer(layers[i], layers[i+1], activation[i - 1], activation[i]))\n",
    "\n",
    "    # forward pass the information through the layers and out the results of final output layer\n",
    "    def forward(self, input, step_type, batch_norm, momentum=None):\n",
    "        for ctr, layer in enumerate(self.layers):\n",
    "            output = layer.forward(input, step_type, batch_norm, momentum)\n",
    "            \n",
    "            if 1 <= ctr <= len(self.layers) - 2 and step_type == 'train':\n",
    "                output = output * np.atleast_2d(np.random.binomial(1, self.dropout, output.shape[0])).T\n",
    "            elif step_type == 'test':\n",
    "                output = output * self.dropout\n",
    "            input = output\n",
    "        return output\n",
    "\n",
    "    # Cross Entropy loss function with softmax\n",
    "    def criterion_softmax_cross(self, y, y_hat):\n",
    "        loss = np.mean(-y * np.log(y_hat + 1e-15))\n",
    "        #calculate the delta of the output layer (simplified due to softmax + CEL combo)\n",
    "        delta = y_hat - y\n",
    "        # return loss and delta\n",
    "        return loss, delta \n",
    "\n",
    "    # backward pass  \n",
    "    def backward(self, delta, batch_norm):\n",
    "        delta = self.layers[-1].backward(delta, batch_norm, output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta = layer.backward(delta, batch_norm)\n",
    "\n",
    "    # update the network weights after backward using SGD with momentum + engineers decay method\n",
    "    def update(self,lr, mom, dec):\n",
    "        for layer in self.layers:\n",
    "            \n",
    "            # calc the accumulated gradient\n",
    "            layer.accgrad_W = mom * layer.accgrad_W + lr * layer.grad_W\n",
    "            layer.accgrad_W = mom * layer.accgrad_W + lr * layer.grad_b\n",
    "            \n",
    "            # Update the weights with accum gradient & decay\n",
    "            layer.W = layer.W - layer.accgrad_W * dec\n",
    "            layer.b = layer.b - layer.accgrad_b * dec\n",
    "    \n",
    "    # define the training function\n",
    "    def fit(self, X, y, X_test, y_test, batch_size, batch_norm=False, learning_rate=0.1, momentum = 0.9, decay = 0.98, epochs=100, dropout=1):\n",
    "        \"\"\"\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :batch_size: Size of batch for mini-batch training\n",
    "        :bath_norm: True/false for turning on/off batch normalisation\n",
    "        :learning_rate: Defines the speed of learning\n",
    "        :momentum: Momentum terma (gamma) \n",
    "        :decay: Engineers weight decay method\n",
    "        :epochs: Number of times the dataset is presented to the network for learning\n",
    "        :dropout: Nodes are droppedout with a probability of p (dropout)\n",
    "        \"\"\"\n",
    "        \n",
    "        # set epsilon to small value for batch normalisation \n",
    "        epsilon = 1e-5\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        X_test = np.array(X_test)\n",
    "        y_test = np.array(y_test)\n",
    "        to_return = [0] * epochs\n",
    "        num_batches = int(X.shape[0] / batch_size)\n",
    "        self.dropout = dropout\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        for k in range(epochs):\n",
    "            loss = np.zeros(num_batches)\n",
    "            indices = np.arange(X.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "    \n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "    \n",
    "            for it in range(num_batches):\n",
    "                # forward pass\n",
    "                y_hat = self.forward(X[it * batch_size: (it + 1) * batch_size], 'train', batch_norm, momentum)\n",
    "\n",
    "                # backward pass\n",
    "                loss[it], delta = self.criterion_softmax_cross(y[it * batch_size: (it + 1) * batch_size], y_hat)\n",
    "                self.backward(delta / batch_size, batch_norm)\n",
    "                y\n",
    "\n",
    "                # update\n",
    "                self.update(learning_rate, momentum, decay)\n",
    "            \n",
    "            # Train performance metrics\n",
    "            pred = self.predict(X, y, 'train', batch_norm)\n",
    "            target = np.squeeze(np.argmax(y, axis=1))\n",
    "\n",
    "            # Accuracy, F1 Score, AUC Score\n",
    "            train_accuracy = accuracy_score(target, pred)\n",
    "            f1 = f1_score(target, pred, average='micro')\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(target, pred, pos_label=2)\n",
    "            AUC = metrics.auc(fpr, tpr)\n",
    "            \n",
    "            # Test performance metrics\n",
    "            test_results = self.predict(X_test, y_test, 'test', batch_norm)\n",
    "            test_pred = test_results[0]\n",
    "            test_target = np.squeeze(np.argmax(np.squeeze(y_test), axis=1))\n",
    "            test_loss = test_results[1]\n",
    "            \n",
    "            # Accuracy, F1 Score, AUC Score\n",
    "            test_accuracy = accuracy_score(test_target, test_pred)\n",
    "            test_f1 = f1_score(test_target, test_pred, average='micro')\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(test_target, test_pred, pos_label=2)\n",
    "            test_AUC = metrics.auc(fpr, tpr)\n",
    "            \n",
    "            # Train & Test Metrics\n",
    "            train_metrics = np.mean(loss), 100 * train_accuracy, f1, AUC\n",
    "            test_metrics = np.mean(test_loss), 100 * test_accuracy, test_f1, test_AUC\n",
    "            \n",
    "            # Return Train & Test Metrics\n",
    "            to_return[k] = train_metrics, test_metrics\n",
    "            \n",
    "            # Print performance for each epoch\n",
    "            print(f'Epoch {k + 1}')\n",
    "            print(f'Training : Loss = {train_metrics[0]:.4f}, Accuracy = {train_metrics[1]:.2f}%, f1_score = {f1:.4f}, AUC = {AUC:.4f}')\n",
    "            print(f'Test     : Loss = {test_metrics[0]:.4f}, Accuracy = {test_metrics[1]:.2f}%, f1_score = {test_f1:.4f}, AUC = {test_AUC:.4f}\\n')\n",
    "        \n",
    "        return np.array(to_return)\n",
    "\n",
    "    # define the prediction function\n",
    "    def predict(self, X, y, data_type, batch_norm):\n",
    "        X = np.array(X)\n",
    "        output = self.forward(X, 'test', batch_norm)\n",
    "\n",
    "        if data_type == 'test':\n",
    "            test_loss = self.criterion_softmax_cross(y, output)[0]\n",
    "\n",
    "        output = np.squeeze(np.argmax(output, axis=1))\n",
    "\n",
    "        if data_type == 'test':\n",
    "            return np.array(output), test_loss\n",
    "\n",
    "        return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training : Loss = 0.8409, Accuracy = 9.61%, f1_score = 0.0961, AUC = 0.4994\n",
      "Test     : Loss = 0.3210, Accuracy = 8.53%, f1_score = 0.0853, AUC = 0.5491\n",
      "\n",
      "Epoch 2\n",
      "Training : Loss = 0.2607, Accuracy = 9.67%, f1_score = 0.0967, AUC = 0.5050\n",
      "Test     : Loss = 0.2682, Accuracy = 8.82%, f1_score = 0.0882, AUC = 0.5425\n",
      "\n",
      "Epoch 3\n",
      "Training : Loss = 0.2595, Accuracy = 9.78%, f1_score = 0.0978, AUC = 0.5034\n",
      "Test     : Loss = 0.2687, Accuracy = 8.82%, f1_score = 0.0882, AUC = 0.5510\n",
      "\n",
      "Epoch 4\n",
      "Training : Loss = 0.2580, Accuracy = 9.76%, f1_score = 0.0976, AUC = 0.5054\n",
      "Test     : Loss = 0.2698, Accuracy = 8.71%, f1_score = 0.0871, AUC = 0.5469\n",
      "\n",
      "Epoch 5\n",
      "Training : Loss = 0.2576, Accuracy = 10.05%, f1_score = 0.1005, AUC = 0.5010\n",
      "Test     : Loss = 0.2680, Accuracy = 8.97%, f1_score = 0.0897, AUC = 0.5456\n",
      "\n",
      "Epoch 6\n",
      "Training : Loss = 0.2556, Accuracy = 10.06%, f1_score = 0.1006, AUC = 0.5068\n",
      "Test     : Loss = 0.2702, Accuracy = 8.86%, f1_score = 0.0886, AUC = 0.5517\n",
      "\n",
      "Epoch 7\n",
      "Training : Loss = 0.2532, Accuracy = 10.13%, f1_score = 0.1013, AUC = 0.5067\n",
      "Test     : Loss = 0.2687, Accuracy = 8.82%, f1_score = 0.0882, AUC = 0.5503\n",
      "\n",
      "Epoch 8\n",
      "Training : Loss = 0.2539, Accuracy = 10.43%, f1_score = 0.1043, AUC = 0.5053\n",
      "Test     : Loss = 0.2660, Accuracy = 9.00%, f1_score = 0.0900, AUC = 0.5448\n",
      "\n",
      "Epoch 9\n",
      "Training : Loss = 0.2515, Accuracy = 10.41%, f1_score = 0.1041, AUC = 0.5098\n",
      "Test     : Loss = 0.2646, Accuracy = 9.16%, f1_score = 0.0916, AUC = 0.5431\n",
      "\n",
      "Epoch 10\n",
      "Training : Loss = 0.2505, Accuracy = 10.50%, f1_score = 0.1050, AUC = 0.5103\n",
      "Test     : Loss = 0.2657, Accuracy = 9.11%, f1_score = 0.0911, AUC = 0.5447\n",
      "\n",
      "Epoch 11\n",
      "Training : Loss = 0.2505, Accuracy = 10.39%, f1_score = 0.1039, AUC = 0.5148\n",
      "Test     : Loss = 0.2677, Accuracy = 9.03%, f1_score = 0.0903, AUC = 0.5386\n",
      "\n",
      "Epoch 12\n",
      "Training : Loss = 0.2489, Accuracy = 10.47%, f1_score = 0.1047, AUC = 0.5179\n",
      "Test     : Loss = 0.2665, Accuracy = 9.25%, f1_score = 0.0925, AUC = 0.5361\n",
      "\n",
      "Epoch 13\n",
      "Training : Loss = 0.2482, Accuracy = 10.56%, f1_score = 0.1056, AUC = 0.5174\n",
      "Test     : Loss = 0.2661, Accuracy = 9.34%, f1_score = 0.0934, AUC = 0.5379\n",
      "\n",
      "Epoch 14\n",
      "Training : Loss = 0.2474, Accuracy = 10.57%, f1_score = 0.1057, AUC = 0.5190\n",
      "Test     : Loss = 0.2696, Accuracy = 9.19%, f1_score = 0.0919, AUC = 0.5330\n",
      "\n",
      "Epoch 15\n",
      "Training : Loss = 0.2472, Accuracy = 10.93%, f1_score = 0.1093, AUC = 0.5124\n",
      "Test     : Loss = 0.2641, Accuracy = 9.46%, f1_score = 0.0946, AUC = 0.5257\n",
      "\n",
      "Epoch 16\n",
      "Training : Loss = 0.2468, Accuracy = 10.83%, f1_score = 0.1083, AUC = 0.5124\n",
      "Test     : Loss = 0.2657, Accuracy = 9.36%, f1_score = 0.0936, AUC = 0.5250\n",
      "\n",
      "Epoch 17\n",
      "Training : Loss = 0.2458, Accuracy = 10.80%, f1_score = 0.1080, AUC = 0.5162\n",
      "Test     : Loss = 0.2658, Accuracy = 9.28%, f1_score = 0.0928, AUC = 0.5274\n",
      "\n",
      "Epoch 18\n",
      "Training : Loss = 0.2452, Accuracy = 10.75%, f1_score = 0.1075, AUC = 0.5195\n",
      "Test     : Loss = 0.2640, Accuracy = 9.49%, f1_score = 0.0949, AUC = 0.5348\n",
      "\n",
      "Epoch 19\n",
      "Training : Loss = 0.2444, Accuracy = 11.06%, f1_score = 0.1106, AUC = 0.5141\n",
      "Test     : Loss = 0.2609, Accuracy = 9.71%, f1_score = 0.0971, AUC = 0.5379\n",
      "\n",
      "Epoch 20\n",
      "Training : Loss = 0.2437, Accuracy = 11.25%, f1_score = 0.1125, AUC = 0.5078\n",
      "Test     : Loss = 0.2653, Accuracy = 9.88%, f1_score = 0.0988, AUC = 0.5352\n",
      "\n",
      "Epoch 21\n",
      "Training : Loss = 0.2432, Accuracy = 11.33%, f1_score = 0.1133, AUC = 0.5078\n",
      "Test     : Loss = 0.2621, Accuracy = 9.77%, f1_score = 0.0977, AUC = 0.5442\n",
      "\n",
      "Epoch 22\n",
      "Training : Loss = 0.2425, Accuracy = 11.08%, f1_score = 0.1108, AUC = 0.5176\n",
      "Test     : Loss = 0.2654, Accuracy = 9.62%, f1_score = 0.0962, AUC = 0.5348\n",
      "\n",
      "Epoch 23\n",
      "Training : Loss = 0.2421, Accuracy = 11.47%, f1_score = 0.1147, AUC = 0.5101\n",
      "Test     : Loss = 0.2625, Accuracy = 9.87%, f1_score = 0.0987, AUC = 0.5431\n",
      "\n",
      "Epoch 24\n",
      "Training : Loss = 0.2413, Accuracy = 11.33%, f1_score = 0.1133, AUC = 0.5113\n",
      "Test     : Loss = 0.2639, Accuracy = 9.80%, f1_score = 0.0980, AUC = 0.5385\n",
      "\n",
      "Epoch 25\n",
      "Training : Loss = 0.2410, Accuracy = 11.56%, f1_score = 0.1156, AUC = 0.5095\n",
      "Test     : Loss = 0.2626, Accuracy = 10.05%, f1_score = 0.1005, AUC = 0.5375\n",
      "\n",
      "Epoch 26\n",
      "Training : Loss = 0.2412, Accuracy = 11.61%, f1_score = 0.1161, AUC = 0.5080\n",
      "Test     : Loss = 0.2627, Accuracy = 10.12%, f1_score = 0.1012, AUC = 0.5312\n",
      "\n",
      "Epoch 27\n",
      "Training : Loss = 0.2402, Accuracy = 11.50%, f1_score = 0.1150, AUC = 0.5070\n",
      "Test     : Loss = 0.2641, Accuracy = 10.17%, f1_score = 0.1017, AUC = 0.5233\n",
      "\n",
      "Epoch 28\n",
      "Training : Loss = 0.2394, Accuracy = 11.82%, f1_score = 0.1182, AUC = 0.5114\n",
      "Test     : Loss = 0.2612, Accuracy = 10.14%, f1_score = 0.1014, AUC = 0.5402\n",
      "\n",
      "Epoch 29\n",
      "Training : Loss = 0.2396, Accuracy = 11.97%, f1_score = 0.1197, AUC = 0.5111\n",
      "Test     : Loss = 0.2614, Accuracy = 10.06%, f1_score = 0.1006, AUC = 0.5389\n",
      "\n",
      "Epoch 30\n",
      "Training : Loss = 0.2393, Accuracy = 11.87%, f1_score = 0.1187, AUC = 0.5114\n",
      "Test     : Loss = 0.2608, Accuracy = 10.17%, f1_score = 0.1017, AUC = 0.5332\n",
      "\n",
      "Epoch 31\n",
      "Training : Loss = 0.2393, Accuracy = 12.11%, f1_score = 0.1211, AUC = 0.5118\n",
      "Test     : Loss = 0.2577, Accuracy = 10.24%, f1_score = 0.1024, AUC = 0.5386\n",
      "\n",
      "Epoch 32\n",
      "Training : Loss = 0.2390, Accuracy = 12.16%, f1_score = 0.1216, AUC = 0.5112\n",
      "Test     : Loss = 0.2603, Accuracy = 10.25%, f1_score = 0.1025, AUC = 0.5405\n",
      "\n",
      "Epoch 33\n",
      "Training : Loss = 0.2389, Accuracy = 12.03%, f1_score = 0.1203, AUC = 0.5106\n",
      "Test     : Loss = 0.2628, Accuracy = 10.44%, f1_score = 0.1044, AUC = 0.5417\n",
      "\n",
      "Epoch 34\n",
      "Training : Loss = 0.2381, Accuracy = 12.17%, f1_score = 0.1217, AUC = 0.5096\n",
      "Test     : Loss = 0.2598, Accuracy = 10.13%, f1_score = 0.1013, AUC = 0.5377\n",
      "\n",
      "Epoch 35\n",
      "Training : Loss = 0.2381, Accuracy = 12.32%, f1_score = 0.1232, AUC = 0.5113\n",
      "Test     : Loss = 0.2596, Accuracy = 10.27%, f1_score = 0.1027, AUC = 0.5399\n",
      "\n",
      "Epoch 36\n",
      "Training : Loss = 0.2384, Accuracy = 12.24%, f1_score = 0.1224, AUC = 0.5118\n",
      "Test     : Loss = 0.2630, Accuracy = 10.45%, f1_score = 0.1045, AUC = 0.5375\n",
      "\n",
      "Epoch 37\n",
      "Training : Loss = 0.2374, Accuracy = 12.56%, f1_score = 0.1256, AUC = 0.5108\n",
      "Test     : Loss = 0.2593, Accuracy = 10.58%, f1_score = 0.1058, AUC = 0.5394\n",
      "\n",
      "Epoch 38\n",
      "Training : Loss = 0.2380, Accuracy = 12.36%, f1_score = 0.1236, AUC = 0.5139\n",
      "Test     : Loss = 0.2595, Accuracy = 10.49%, f1_score = 0.1049, AUC = 0.5383\n",
      "\n",
      "Epoch 39\n",
      "Training : Loss = 0.2375, Accuracy = 12.54%, f1_score = 0.1254, AUC = 0.5129\n",
      "Test     : Loss = 0.2627, Accuracy = 10.51%, f1_score = 0.1051, AUC = 0.5319\n",
      "\n",
      "Epoch 40\n",
      "Training : Loss = 0.2374, Accuracy = 12.76%, f1_score = 0.1276, AUC = 0.5052\n",
      "Test     : Loss = 0.2599, Accuracy = 10.90%, f1_score = 0.1090, AUC = 0.5266\n",
      "\n",
      "Epoch 41\n",
      "Training : Loss = 0.2373, Accuracy = 12.77%, f1_score = 0.1277, AUC = 0.5105\n",
      "Test     : Loss = 0.2586, Accuracy = 10.67%, f1_score = 0.1067, AUC = 0.5297\n",
      "\n",
      "Epoch 42\n",
      "Training : Loss = 0.2370, Accuracy = 12.79%, f1_score = 0.1279, AUC = 0.5073\n",
      "Test     : Loss = 0.2595, Accuracy = 10.86%, f1_score = 0.1086, AUC = 0.5334\n",
      "\n",
      "Epoch 43\n",
      "Training : Loss = 0.2370, Accuracy = 13.06%, f1_score = 0.1306, AUC = 0.5051\n",
      "Test     : Loss = 0.2578, Accuracy = 11.04%, f1_score = 0.1104, AUC = 0.5236\n",
      "\n",
      "Epoch 44\n",
      "Training : Loss = 0.2363, Accuracy = 13.05%, f1_score = 0.1305, AUC = 0.5042\n",
      "Test     : Loss = 0.2579, Accuracy = 10.91%, f1_score = 0.1091, AUC = 0.5250\n",
      "\n",
      "Epoch 45\n",
      "Training : Loss = 0.2369, Accuracy = 13.03%, f1_score = 0.1303, AUC = 0.5061\n",
      "Test     : Loss = 0.2585, Accuracy = 10.91%, f1_score = 0.1091, AUC = 0.5237\n",
      "\n",
      "Epoch 46\n",
      "Training : Loss = 0.2361, Accuracy = 12.90%, f1_score = 0.1290, AUC = 0.5101\n",
      "Test     : Loss = 0.2604, Accuracy = 11.04%, f1_score = 0.1104, AUC = 0.5260\n",
      "\n",
      "Epoch 47\n",
      "Training : Loss = 0.2361, Accuracy = 13.03%, f1_score = 0.1303, AUC = 0.5037\n",
      "Test     : Loss = 0.2595, Accuracy = 10.86%, f1_score = 0.1086, AUC = 0.5157\n",
      "\n",
      "Epoch 48\n",
      "Training : Loss = 0.2363, Accuracy = 12.91%, f1_score = 0.1291, AUC = 0.5097\n",
      "Test     : Loss = 0.2598, Accuracy = 10.98%, f1_score = 0.1098, AUC = 0.5256\n",
      "\n",
      "Epoch 49\n",
      "Training : Loss = 0.2356, Accuracy = 13.41%, f1_score = 0.1341, AUC = 0.5007\n",
      "Test     : Loss = 0.2561, Accuracy = 11.21%, f1_score = 0.1121, AUC = 0.5170\n",
      "\n",
      "Epoch 50\n",
      "Training : Loss = 0.2359, Accuracy = 13.38%, f1_score = 0.1338, AUC = 0.5020\n",
      "Test     : Loss = 0.2570, Accuracy = 11.21%, f1_score = 0.1121, AUC = 0.5135\n",
      "\n",
      "Epoch 51\n",
      "Training : Loss = 0.2352, Accuracy = 13.40%, f1_score = 0.1340, AUC = 0.5008\n",
      "Test     : Loss = 0.2579, Accuracy = 11.38%, f1_score = 0.1138, AUC = 0.5163\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52\n",
      "Training : Loss = 0.2356, Accuracy = 13.33%, f1_score = 0.1333, AUC = 0.5018\n",
      "Test     : Loss = 0.2588, Accuracy = 11.15%, f1_score = 0.1115, AUC = 0.5184\n",
      "\n",
      "Epoch 53\n",
      "Training : Loss = 0.2352, Accuracy = 13.52%, f1_score = 0.1352, AUC = 0.5012\n",
      "Test     : Loss = 0.2573, Accuracy = 11.26%, f1_score = 0.1126, AUC = 0.5220\n",
      "\n",
      "Epoch 54\n",
      "Training : Loss = 0.2351, Accuracy = 13.54%, f1_score = 0.1354, AUC = 0.5045\n",
      "Test     : Loss = 0.2564, Accuracy = 11.45%, f1_score = 0.1145, AUC = 0.5180\n",
      "\n",
      "Epoch 55\n",
      "Training : Loss = 0.2349, Accuracy = 13.54%, f1_score = 0.1354, AUC = 0.5025\n",
      "Test     : Loss = 0.2578, Accuracy = 11.56%, f1_score = 0.1156, AUC = 0.5120\n",
      "\n",
      "Epoch 56\n",
      "Training : Loss = 0.2347, Accuracy = 13.67%, f1_score = 0.1367, AUC = 0.5017\n",
      "Test     : Loss = 0.2563, Accuracy = 11.66%, f1_score = 0.1166, AUC = 0.5129\n",
      "\n",
      "Epoch 57\n",
      "Training : Loss = 0.2346, Accuracy = 13.71%, f1_score = 0.1371, AUC = 0.5015\n",
      "Test     : Loss = 0.2573, Accuracy = 11.39%, f1_score = 0.1139, AUC = 0.5132\n",
      "\n",
      "Epoch 58\n",
      "Training : Loss = 0.2344, Accuracy = 13.67%, f1_score = 0.1367, AUC = 0.5013\n",
      "Test     : Loss = 0.2557, Accuracy = 11.47%, f1_score = 0.1147, AUC = 0.5100\n",
      "\n",
      "Epoch 59\n",
      "Training : Loss = 0.2343, Accuracy = 13.69%, f1_score = 0.1369, AUC = 0.5060\n",
      "Test     : Loss = 0.2563, Accuracy = 11.70%, f1_score = 0.1170, AUC = 0.5159\n",
      "\n",
      "Epoch 60\n",
      "Training : Loss = 0.2339, Accuracy = 13.80%, f1_score = 0.1380, AUC = 0.5010\n",
      "Test     : Loss = 0.2556, Accuracy = 11.76%, f1_score = 0.1176, AUC = 0.5057\n",
      "\n",
      "Epoch 61\n",
      "Training : Loss = 0.2338, Accuracy = 13.78%, f1_score = 0.1378, AUC = 0.5038\n",
      "Test     : Loss = 0.2573, Accuracy = 11.72%, f1_score = 0.1172, AUC = 0.5213\n",
      "\n",
      "Epoch 62\n",
      "Training : Loss = 0.2337, Accuracy = 14.10%, f1_score = 0.1410, AUC = 0.5026\n",
      "Test     : Loss = 0.2565, Accuracy = 11.81%, f1_score = 0.1181, AUC = 0.5208\n",
      "\n",
      "Epoch 63\n",
      "Training : Loss = 0.2336, Accuracy = 13.91%, f1_score = 0.1391, AUC = 0.5039\n",
      "Test     : Loss = 0.2574, Accuracy = 11.91%, f1_score = 0.1191, AUC = 0.5158\n",
      "\n",
      "Epoch 64\n",
      "Training : Loss = 0.2334, Accuracy = 14.18%, f1_score = 0.1418, AUC = 0.4981\n",
      "Test     : Loss = 0.2570, Accuracy = 11.98%, f1_score = 0.1198, AUC = 0.5090\n",
      "\n",
      "Epoch 65\n",
      "Training : Loss = 0.2334, Accuracy = 14.12%, f1_score = 0.1412, AUC = 0.5010\n",
      "Test     : Loss = 0.2558, Accuracy = 12.09%, f1_score = 0.1209, AUC = 0.5064\n",
      "\n",
      "Epoch 66\n",
      "Training : Loss = 0.2331, Accuracy = 14.07%, f1_score = 0.1407, AUC = 0.5018\n",
      "Test     : Loss = 0.2582, Accuracy = 11.73%, f1_score = 0.1173, AUC = 0.5154\n",
      "\n",
      "Epoch 67\n",
      "Training : Loss = 0.2332, Accuracy = 14.23%, f1_score = 0.1423, AUC = 0.4994\n",
      "Test     : Loss = 0.2578, Accuracy = 12.01%, f1_score = 0.1201, AUC = 0.5122\n",
      "\n",
      "Epoch 68\n",
      "Training : Loss = 0.2326, Accuracy = 14.29%, f1_score = 0.1429, AUC = 0.5042\n",
      "Test     : Loss = 0.2549, Accuracy = 12.12%, f1_score = 0.1212, AUC = 0.5042\n",
      "\n",
      "Epoch 69\n",
      "Training : Loss = 0.2325, Accuracy = 14.60%, f1_score = 0.1460, AUC = 0.5005\n",
      "Test     : Loss = 0.2533, Accuracy = 12.25%, f1_score = 0.1225, AUC = 0.5123\n",
      "\n",
      "Epoch 70\n",
      "Training : Loss = 0.2327, Accuracy = 14.58%, f1_score = 0.1458, AUC = 0.5024\n",
      "Test     : Loss = 0.2540, Accuracy = 12.19%, f1_score = 0.1219, AUC = 0.5149\n",
      "\n",
      "Epoch 71\n",
      "Training : Loss = 0.2326, Accuracy = 14.59%, f1_score = 0.1459, AUC = 0.4966\n",
      "Test     : Loss = 0.2549, Accuracy = 12.24%, f1_score = 0.1224, AUC = 0.5075\n",
      "\n",
      "Epoch 72\n",
      "Training : Loss = 0.2324, Accuracy = 14.59%, f1_score = 0.1459, AUC = 0.5003\n",
      "Test     : Loss = 0.2546, Accuracy = 12.40%, f1_score = 0.1240, AUC = 0.5073\n",
      "\n",
      "Epoch 73\n",
      "Training : Loss = 0.2325, Accuracy = 14.66%, f1_score = 0.1466, AUC = 0.5043\n",
      "Test     : Loss = 0.2539, Accuracy = 12.42%, f1_score = 0.1242, AUC = 0.5117\n",
      "\n",
      "Epoch 74\n",
      "Training : Loss = 0.2319, Accuracy = 14.75%, f1_score = 0.1475, AUC = 0.5036\n",
      "Test     : Loss = 0.2537, Accuracy = 12.32%, f1_score = 0.1232, AUC = 0.5151\n",
      "\n",
      "Epoch 75\n",
      "Training : Loss = 0.2321, Accuracy = 14.67%, f1_score = 0.1467, AUC = 0.5032\n",
      "Test     : Loss = 0.2547, Accuracy = 12.50%, f1_score = 0.1250, AUC = 0.5022\n",
      "\n",
      "Epoch 76\n",
      "Training : Loss = 0.2319, Accuracy = 14.98%, f1_score = 0.1498, AUC = 0.5013\n",
      "Test     : Loss = 0.2563, Accuracy = 12.34%, f1_score = 0.1234, AUC = 0.5168\n",
      "\n",
      "Epoch 77\n",
      "Training : Loss = 0.2320, Accuracy = 14.90%, f1_score = 0.1490, AUC = 0.4984\n",
      "Test     : Loss = 0.2546, Accuracy = 12.70%, f1_score = 0.1270, AUC = 0.5069\n",
      "\n",
      "Epoch 78\n",
      "Training : Loss = 0.2315, Accuracy = 14.74%, f1_score = 0.1474, AUC = 0.5032\n",
      "Test     : Loss = 0.2543, Accuracy = 12.54%, f1_score = 0.1254, AUC = 0.5069\n",
      "\n",
      "Epoch 79\n",
      "Training : Loss = 0.2313, Accuracy = 15.02%, f1_score = 0.1502, AUC = 0.5038\n",
      "Test     : Loss = 0.2536, Accuracy = 12.45%, f1_score = 0.1245, AUC = 0.5145\n",
      "\n",
      "Epoch 80\n",
      "Training : Loss = 0.2310, Accuracy = 15.09%, f1_score = 0.1509, AUC = 0.5017\n",
      "Test     : Loss = 0.2534, Accuracy = 12.56%, f1_score = 0.1256, AUC = 0.5124\n",
      "\n",
      "Epoch 81\n",
      "Training : Loss = 0.2312, Accuracy = 15.08%, f1_score = 0.1508, AUC = 0.4980\n",
      "Test     : Loss = 0.2550, Accuracy = 12.55%, f1_score = 0.1255, AUC = 0.5089\n",
      "\n",
      "Epoch 82\n",
      "Training : Loss = 0.2309, Accuracy = 15.22%, f1_score = 0.1522, AUC = 0.5051\n",
      "Test     : Loss = 0.2534, Accuracy = 12.66%, f1_score = 0.1266, AUC = 0.5086\n",
      "\n",
      "Epoch 83\n",
      "Training : Loss = 0.2307, Accuracy = 15.15%, f1_score = 0.1515, AUC = 0.5021\n",
      "Test     : Loss = 0.2556, Accuracy = 12.81%, f1_score = 0.1281, AUC = 0.5080\n",
      "\n",
      "Epoch 84\n",
      "Training : Loss = 0.2306, Accuracy = 15.40%, f1_score = 0.1540, AUC = 0.4986\n",
      "Test     : Loss = 0.2529, Accuracy = 12.98%, f1_score = 0.1298, AUC = 0.5092\n",
      "\n",
      "Epoch 85\n",
      "Training : Loss = 0.2307, Accuracy = 15.26%, f1_score = 0.1526, AUC = 0.4982\n",
      "Test     : Loss = 0.2546, Accuracy = 12.81%, f1_score = 0.1281, AUC = 0.5033\n",
      "\n",
      "Epoch 86\n",
      "Training : Loss = 0.2306, Accuracy = 15.44%, f1_score = 0.1544, AUC = 0.5080\n",
      "Test     : Loss = 0.2525, Accuracy = 12.84%, f1_score = 0.1284, AUC = 0.5181\n",
      "\n",
      "Epoch 87\n",
      "Training : Loss = 0.2303, Accuracy = 15.46%, f1_score = 0.1546, AUC = 0.4931\n",
      "Test     : Loss = 0.2544, Accuracy = 12.81%, f1_score = 0.1281, AUC = 0.5010\n",
      "\n",
      "Epoch 88\n",
      "Training : Loss = 0.2302, Accuracy = 15.54%, f1_score = 0.1554, AUC = 0.5040\n",
      "Test     : Loss = 0.2531, Accuracy = 12.71%, f1_score = 0.1271, AUC = 0.5152\n",
      "\n",
      "Epoch 89\n",
      "Training : Loss = 0.2302, Accuracy = 15.61%, f1_score = 0.1561, AUC = 0.4961\n",
      "Test     : Loss = 0.2539, Accuracy = 12.85%, f1_score = 0.1285, AUC = 0.5126\n",
      "\n",
      "Epoch 90\n",
      "Training : Loss = 0.2299, Accuracy = 15.96%, f1_score = 0.1596, AUC = 0.4994\n",
      "Test     : Loss = 0.2507, Accuracy = 13.35%, f1_score = 0.1335, AUC = 0.5085\n",
      "\n",
      "Epoch 91\n",
      "Training : Loss = 0.2299, Accuracy = 15.70%, f1_score = 0.1570, AUC = 0.4996\n",
      "Test     : Loss = 0.2542, Accuracy = 12.93%, f1_score = 0.1293, AUC = 0.5112\n",
      "\n",
      "Epoch 92\n",
      "Training : Loss = 0.2297, Accuracy = 15.85%, f1_score = 0.1585, AUC = 0.5015\n",
      "Test     : Loss = 0.2530, Accuracy = 13.14%, f1_score = 0.1314, AUC = 0.5135\n",
      "\n",
      "Epoch 93\n",
      "Training : Loss = 0.2295, Accuracy = 15.89%, f1_score = 0.1589, AUC = 0.4981\n",
      "Test     : Loss = 0.2519, Accuracy = 13.39%, f1_score = 0.1339, AUC = 0.5057\n",
      "\n",
      "Epoch 94\n",
      "Training : Loss = 0.2291, Accuracy = 16.07%, f1_score = 0.1607, AUC = 0.5007\n",
      "Test     : Loss = 0.2517, Accuracy = 13.34%, f1_score = 0.1334, AUC = 0.5052\n",
      "\n",
      "Epoch 95\n",
      "Training : Loss = 0.2294, Accuracy = 15.99%, f1_score = 0.1599, AUC = 0.5001\n",
      "Test     : Loss = 0.2528, Accuracy = 13.22%, f1_score = 0.1322, AUC = 0.5112\n",
      "\n",
      "Epoch 96\n",
      "Training : Loss = 0.2291, Accuracy = 16.07%, f1_score = 0.1607, AUC = 0.4926\n",
      "Test     : Loss = 0.2530, Accuracy = 13.32%, f1_score = 0.1332, AUC = 0.5029\n",
      "\n",
      "Epoch 97\n",
      "Training : Loss = 0.2294, Accuracy = 16.06%, f1_score = 0.1606, AUC = 0.5006\n",
      "Test     : Loss = 0.2522, Accuracy = 13.44%, f1_score = 0.1344, AUC = 0.5037\n",
      "\n",
      "Epoch 98\n",
      "Training : Loss = 0.2291, Accuracy = 16.10%, f1_score = 0.1610, AUC = 0.4946\n",
      "Test     : Loss = 0.2524, Accuracy = 13.55%, f1_score = 0.1355, AUC = 0.5022\n",
      "\n",
      "Epoch 99\n",
      "Training : Loss = 0.2288, Accuracy = 16.36%, f1_score = 0.1636, AUC = 0.4942\n",
      "Test     : Loss = 0.2516, Accuracy = 13.66%, f1_score = 0.1366, AUC = 0.5013\n",
      "\n",
      "Epoch 100\n",
      "Training : Loss = 0.2289, Accuracy = 16.30%, f1_score = 0.1630, AUC = 0.4988\n",
      "Test     : Loss = 0.2514, Accuracy = 13.54%, f1_score = 0.1354, AUC = 0.5092\n",
      "\n",
      "Epoch 101\n",
      "Training : Loss = 0.2286, Accuracy = 16.38%, f1_score = 0.1638, AUC = 0.4977\n",
      "Test     : Loss = 0.2509, Accuracy = 13.69%, f1_score = 0.1369, AUC = 0.4936\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102\n",
      "Training : Loss = 0.2287, Accuracy = 16.32%, f1_score = 0.1632, AUC = 0.4968\n",
      "Test     : Loss = 0.2506, Accuracy = 13.74%, f1_score = 0.1374, AUC = 0.4947\n",
      "\n",
      "Epoch 103\n",
      "Training : Loss = 0.2284, Accuracy = 16.37%, f1_score = 0.1637, AUC = 0.4922\n",
      "Test     : Loss = 0.2521, Accuracy = 13.60%, f1_score = 0.1360, AUC = 0.5012\n",
      "\n",
      "Epoch 104\n",
      "Training : Loss = 0.2282, Accuracy = 16.33%, f1_score = 0.1633, AUC = 0.4957\n",
      "Test     : Loss = 0.2519, Accuracy = 13.86%, f1_score = 0.1386, AUC = 0.4987\n",
      "\n",
      "Epoch 105\n",
      "Training : Loss = 0.2282, Accuracy = 16.56%, f1_score = 0.1656, AUC = 0.4895\n",
      "Test     : Loss = 0.2513, Accuracy = 13.84%, f1_score = 0.1384, AUC = 0.4940\n",
      "\n",
      "Epoch 106\n",
      "Training : Loss = 0.2280, Accuracy = 16.51%, f1_score = 0.1651, AUC = 0.4951\n",
      "Test     : Loss = 0.2518, Accuracy = 14.02%, f1_score = 0.1402, AUC = 0.4953\n",
      "\n",
      "Epoch 107\n",
      "Training : Loss = 0.2281, Accuracy = 16.64%, f1_score = 0.1664, AUC = 0.4891\n",
      "Test     : Loss = 0.2504, Accuracy = 14.09%, f1_score = 0.1409, AUC = 0.4919\n",
      "\n",
      "Epoch 108\n",
      "Training : Loss = 0.2278, Accuracy = 16.72%, f1_score = 0.1672, AUC = 0.4915\n",
      "Test     : Loss = 0.2516, Accuracy = 14.02%, f1_score = 0.1402, AUC = 0.4935\n",
      "\n",
      "Epoch 109\n",
      "Training : Loss = 0.2278, Accuracy = 16.52%, f1_score = 0.1652, AUC = 0.4919\n",
      "Test     : Loss = 0.2528, Accuracy = 13.92%, f1_score = 0.1392, AUC = 0.5000\n",
      "\n",
      "Epoch 110\n",
      "Training : Loss = 0.2275, Accuracy = 16.71%, f1_score = 0.1671, AUC = 0.4832\n",
      "Test     : Loss = 0.2519, Accuracy = 14.25%, f1_score = 0.1425, AUC = 0.4844\n",
      "\n",
      "Epoch 111\n",
      "Training : Loss = 0.2277, Accuracy = 16.92%, f1_score = 0.1692, AUC = 0.4882\n",
      "Test     : Loss = 0.2504, Accuracy = 14.28%, f1_score = 0.1428, AUC = 0.4963\n",
      "\n",
      "Epoch 112\n",
      "Training : Loss = 0.2276, Accuracy = 16.91%, f1_score = 0.1691, AUC = 0.4869\n",
      "Test     : Loss = 0.2495, Accuracy = 14.41%, f1_score = 0.1441, AUC = 0.4913\n",
      "\n",
      "Epoch 113\n",
      "Training : Loss = 0.2271, Accuracy = 16.93%, f1_score = 0.1693, AUC = 0.4939\n",
      "Test     : Loss = 0.2518, Accuracy = 14.16%, f1_score = 0.1416, AUC = 0.5002\n",
      "\n",
      "Epoch 114\n",
      "Training : Loss = 0.2274, Accuracy = 16.90%, f1_score = 0.1690, AUC = 0.4868\n",
      "Test     : Loss = 0.2517, Accuracy = 14.43%, f1_score = 0.1443, AUC = 0.4892\n",
      "\n",
      "Epoch 115\n",
      "Training : Loss = 0.2271, Accuracy = 17.16%, f1_score = 0.1716, AUC = 0.4810\n",
      "Test     : Loss = 0.2517, Accuracy = 14.52%, f1_score = 0.1452, AUC = 0.4943\n",
      "\n",
      "Epoch 116\n",
      "Training : Loss = 0.2270, Accuracy = 17.10%, f1_score = 0.1710, AUC = 0.4921\n",
      "Test     : Loss = 0.2496, Accuracy = 14.51%, f1_score = 0.1451, AUC = 0.4886\n",
      "\n",
      "Epoch 117\n",
      "Training : Loss = 0.2269, Accuracy = 17.13%, f1_score = 0.1713, AUC = 0.4868\n",
      "Test     : Loss = 0.2523, Accuracy = 14.36%, f1_score = 0.1436, AUC = 0.4896\n",
      "\n",
      "Epoch 118\n",
      "Training : Loss = 0.2269, Accuracy = 17.32%, f1_score = 0.1732, AUC = 0.4882\n",
      "Test     : Loss = 0.2512, Accuracy = 14.61%, f1_score = 0.1461, AUC = 0.4936\n",
      "\n",
      "Epoch 119\n",
      "Training : Loss = 0.2265, Accuracy = 17.39%, f1_score = 0.1739, AUC = 0.4811\n",
      "Test     : Loss = 0.2525, Accuracy = 14.40%, f1_score = 0.1440, AUC = 0.4927\n",
      "\n",
      "Epoch 120\n",
      "Training : Loss = 0.2268, Accuracy = 17.46%, f1_score = 0.1746, AUC = 0.4847\n",
      "Test     : Loss = 0.2491, Accuracy = 14.83%, f1_score = 0.1483, AUC = 0.4881\n",
      "\n",
      "Epoch 121\n",
      "Training : Loss = 0.2266, Accuracy = 17.35%, f1_score = 0.1735, AUC = 0.4887\n",
      "Test     : Loss = 0.2533, Accuracy = 14.66%, f1_score = 0.1466, AUC = 0.4899\n",
      "\n",
      "Epoch 122\n",
      "Training : Loss = 0.2267, Accuracy = 17.49%, f1_score = 0.1749, AUC = 0.4849\n",
      "Test     : Loss = 0.2523, Accuracy = 14.49%, f1_score = 0.1449, AUC = 0.4843\n",
      "\n",
      "Epoch 123\n",
      "Training : Loss = 0.2264, Accuracy = 17.72%, f1_score = 0.1772, AUC = 0.4844\n",
      "Test     : Loss = 0.2533, Accuracy = 14.79%, f1_score = 0.1479, AUC = 0.4884\n",
      "\n",
      "Epoch 124\n",
      "Training : Loss = 0.2262, Accuracy = 17.75%, f1_score = 0.1775, AUC = 0.4793\n",
      "Test     : Loss = 0.2531, Accuracy = 14.61%, f1_score = 0.1461, AUC = 0.4895\n",
      "\n",
      "Epoch 125\n",
      "Training : Loss = 0.2259, Accuracy = 17.79%, f1_score = 0.1779, AUC = 0.4807\n",
      "Test     : Loss = 0.2521, Accuracy = 14.89%, f1_score = 0.1489, AUC = 0.4832\n",
      "\n",
      "Epoch 126\n",
      "Training : Loss = 0.2260, Accuracy = 17.72%, f1_score = 0.1772, AUC = 0.4854\n",
      "Test     : Loss = 0.2527, Accuracy = 14.64%, f1_score = 0.1464, AUC = 0.4883\n",
      "\n",
      "Epoch 127\n",
      "Training : Loss = 0.2258, Accuracy = 17.80%, f1_score = 0.1780, AUC = 0.4834\n",
      "Test     : Loss = 0.2531, Accuracy = 15.03%, f1_score = 0.1503, AUC = 0.4891\n",
      "\n",
      "Epoch 128\n",
      "Training : Loss = 0.2256, Accuracy = 17.79%, f1_score = 0.1779, AUC = 0.4878\n",
      "Test     : Loss = 0.2539, Accuracy = 15.08%, f1_score = 0.1508, AUC = 0.4834\n",
      "\n",
      "Epoch 129\n",
      "Training : Loss = 0.2257, Accuracy = 18.09%, f1_score = 0.1809, AUC = 0.4849\n",
      "Test     : Loss = 0.2527, Accuracy = 15.08%, f1_score = 0.1508, AUC = 0.4888\n",
      "\n",
      "Epoch 130\n",
      "Training : Loss = 0.2255, Accuracy = 18.14%, f1_score = 0.1814, AUC = 0.4820\n",
      "Test     : Loss = 0.2530, Accuracy = 15.22%, f1_score = 0.1522, AUC = 0.4898\n",
      "\n",
      "Epoch 131\n",
      "Training : Loss = 0.2255, Accuracy = 18.02%, f1_score = 0.1802, AUC = 0.4865\n",
      "Test     : Loss = 0.2530, Accuracy = 15.09%, f1_score = 0.1509, AUC = 0.4878\n",
      "\n",
      "Epoch 132\n",
      "Training : Loss = 0.2255, Accuracy = 18.24%, f1_score = 0.1824, AUC = 0.4808\n",
      "Test     : Loss = 0.2529, Accuracy = 15.22%, f1_score = 0.1522, AUC = 0.4882\n",
      "\n",
      "Epoch 133\n",
      "Training : Loss = 0.2255, Accuracy = 18.14%, f1_score = 0.1814, AUC = 0.4859\n",
      "Test     : Loss = 0.2528, Accuracy = 15.39%, f1_score = 0.1539, AUC = 0.4844\n",
      "\n",
      "Epoch 134\n",
      "Training : Loss = 0.2253, Accuracy = 18.45%, f1_score = 0.1845, AUC = 0.4832\n",
      "Test     : Loss = 0.2528, Accuracy = 15.36%, f1_score = 0.1536, AUC = 0.4834\n",
      "\n",
      "Epoch 135\n",
      "Training : Loss = 0.2252, Accuracy = 18.43%, f1_score = 0.1843, AUC = 0.4830\n",
      "Test     : Loss = 0.2529, Accuracy = 15.30%, f1_score = 0.1530, AUC = 0.4850\n",
      "\n",
      "Epoch 136\n",
      "Training : Loss = 0.2252, Accuracy = 18.41%, f1_score = 0.1841, AUC = 0.4883\n",
      "Test     : Loss = 0.2542, Accuracy = 15.29%, f1_score = 0.1529, AUC = 0.4888\n",
      "\n",
      "Epoch 137\n",
      "Training : Loss = 0.2249, Accuracy = 18.37%, f1_score = 0.1837, AUC = 0.4811\n",
      "Test     : Loss = 0.2539, Accuracy = 15.61%, f1_score = 0.1561, AUC = 0.4801\n",
      "\n",
      "Epoch 138\n",
      "Training : Loss = 0.2250, Accuracy = 18.67%, f1_score = 0.1867, AUC = 0.4795\n",
      "Test     : Loss = 0.2539, Accuracy = 15.65%, f1_score = 0.1565, AUC = 0.4782\n",
      "\n",
      "Epoch 139\n",
      "Training : Loss = 0.2253, Accuracy = 18.50%, f1_score = 0.1850, AUC = 0.4843\n",
      "Test     : Loss = 0.2544, Accuracy = 15.43%, f1_score = 0.1543, AUC = 0.4806\n",
      "\n",
      "Epoch 140\n",
      "Training : Loss = 0.2250, Accuracy = 18.72%, f1_score = 0.1872, AUC = 0.4828\n",
      "Test     : Loss = 0.2552, Accuracy = 15.59%, f1_score = 0.1559, AUC = 0.4769\n",
      "\n",
      "Epoch 141\n",
      "Training : Loss = 0.2249, Accuracy = 18.85%, f1_score = 0.1885, AUC = 0.4850\n",
      "Test     : Loss = 0.2531, Accuracy = 15.88%, f1_score = 0.1588, AUC = 0.4806\n",
      "\n",
      "Epoch 142\n",
      "Training : Loss = 0.2249, Accuracy = 18.74%, f1_score = 0.1874, AUC = 0.4896\n",
      "Test     : Loss = 0.2531, Accuracy = 15.89%, f1_score = 0.1589, AUC = 0.4819\n",
      "\n",
      "Epoch 143\n",
      "Training : Loss = 0.2250, Accuracy = 18.82%, f1_score = 0.1882, AUC = 0.4842\n",
      "Test     : Loss = 0.2546, Accuracy = 15.76%, f1_score = 0.1576, AUC = 0.4780\n",
      "\n",
      "Epoch 144\n",
      "Training : Loss = 0.2250, Accuracy = 18.91%, f1_score = 0.1891, AUC = 0.4779\n",
      "Test     : Loss = 0.2559, Accuracy = 15.70%, f1_score = 0.1570, AUC = 0.4755\n",
      "\n",
      "Epoch 145\n",
      "Training : Loss = 0.2249, Accuracy = 18.94%, f1_score = 0.1894, AUC = 0.4839\n",
      "Test     : Loss = 0.2542, Accuracy = 16.24%, f1_score = 0.1624, AUC = 0.4725\n",
      "\n",
      "Epoch 146\n",
      "Training : Loss = 0.2247, Accuracy = 18.94%, f1_score = 0.1894, AUC = 0.4817\n",
      "Test     : Loss = 0.2545, Accuracy = 16.15%, f1_score = 0.1615, AUC = 0.4721\n",
      "\n",
      "Epoch 147\n",
      "Training : Loss = 0.2246, Accuracy = 19.16%, f1_score = 0.1916, AUC = 0.4768\n",
      "Test     : Loss = 0.2553, Accuracy = 16.01%, f1_score = 0.1601, AUC = 0.4765\n",
      "\n",
      "Epoch 148\n",
      "Training : Loss = 0.2247, Accuracy = 19.11%, f1_score = 0.1911, AUC = 0.4816\n",
      "Test     : Loss = 0.2572, Accuracy = 15.83%, f1_score = 0.1583, AUC = 0.4763\n",
      "\n",
      "Epoch 149\n",
      "Training : Loss = 0.2247, Accuracy = 19.27%, f1_score = 0.1927, AUC = 0.4799\n",
      "Test     : Loss = 0.2552, Accuracy = 16.29%, f1_score = 0.1629, AUC = 0.4749\n",
      "\n",
      "Epoch 150\n",
      "Training : Loss = 0.2244, Accuracy = 19.33%, f1_score = 0.1933, AUC = 0.4769\n",
      "Test     : Loss = 0.2577, Accuracy = 16.05%, f1_score = 0.1605, AUC = 0.4736\n",
      "\n",
      "Run Time 9 minutes, 41.7 seconds\n"
     ]
    }
   ],
   "source": [
    "# Record start time\n",
    "start_time = time()\n",
    "\n",
    "batch_norm = False\n",
    "\n",
    "### Try different MLP models\n",
    "nn = MLP([128, 90, 90, 10], [None, 'relu', 'relu','softmax'], batch_norm=batch_norm)\n",
    "\n",
    "input_data = train_data\n",
    "output_data = one_hot_targets\n",
    "\n",
    "### Try different learning rate and e\n",
    "CE = nn.fit(input_data, output_data, test_data, one_hot_targets_test, batch_size=1024, batch_norm=batch_norm, learning_rate=0.001, momentum = 0.9, decay = 0.98, epochs=150, dropout=1)\n",
    "# print('loss:%f'%CE[-1])\n",
    "\n",
    "# Record runtime\n",
    "run_time = timeValue(time() - start_time)\n",
    "print(f'Run Time {run_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss Plot\n",
    "loss_y1 = CE[:, 0, 0]\n",
    "loss_y2 = CE[:, 1, 0]\n",
    "loss_x = np.arange(1, len(CE) + 1)\n",
    "\n",
    "pl.figure(figsize=(5,4))\n",
    "pl.xlabel('Epoch')\n",
    "pl.ylabel('Loss')\n",
    "pl.plot(loss_x, loss_y1, label=\"Training\")\n",
    "pl.plot(loss_x, loss_y2, label=\"Test\")\n",
    "pl.legend()\n",
    "#pl.savefig('Half_Epoch_Loss.png')\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Accuracy Plot\n",
    "acc_y1 = CE[:, 0, 1]\n",
    "acc_y2 = CE[:, 1, 1]\n",
    "acc_x = np.arange(1, len(CE) + 1)\n",
    "\n",
    "pl.figure(figsize=(5,4))\n",
    "pl.xlabel('Epoch')\n",
    "pl.ylabel('Accuracy')\n",
    "pl.plot(acc_x, acc_y1, label=\"Training\")\n",
    "pl.plot(acc_x, acc_y2, label=\"Test\")\n",
    "pl.legend()\n",
    "#pl.savefig('Half_Epoch_Accuracy.png')\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
